import findspark
findspark.init('/home/nakul/.local/lib/python3.6/site-packages/pyspark')
from pyspark.sql import SQLContext
from pyspark import SparkContext
sc=SparkContext("local[*]")

sqlsc=SQLContext(sc)

country_lines = sc.textFile("/home/nakul/Downloads/big-data-3/final-project/country-list.csv")

df.printSchema()

df.count()

df.show(5)

df.select("userId","teamLevel").show(5)

df.filter(df["teamLevel"]>1).select("userid","teamLevel").show(10)
df.groupBy("ishit").count().show()

from pyspark.sql.functions import*
df.select(mean("ishit"),sum("ishit")).show()

df2 = sqlsc.read.format("csv").option("header", "true").load("/home/nakul/Downloads/big-data-3/ad-clicks.csv")

df2.printSchema()

merged=df.join(df2,"userid")

merged.printSchema()
